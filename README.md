# Twitter Search

The repository contains the code for searching users on Twitter based on a given type of account and location. The process searches for tweets related to the account type and location, extracts the users, and filters them based on relevance by using a zero-shot model.

## Setting up the repository

1. Clone the repository

```
git clone https://github.com/Energy-Lab-Pollution/twitter_search.git
```

2. Navigate to the repository

```
cd ./twitter_search
```


3. Download Poetry, which allows the user to run the application in a virtual environment, [following these instructions](https://python-poetry.org/docs/). Then install poetry.

```
poetry install
```

4. Activate the virtual environment in poetry.

```
poetry shell
```
**Note:**  Receive the secret keys from the authors, go to twitter_search/config_utils, and create a file called config.py. You then need to paste all of the access keys there.

## Extracting and classificating users

As of now, we can extract users from Twitter in two ways:

1. Use the [Twikit](https://github.com/d60/twikit) library for scraping.
2. Use [Twitter's](https://docs.x.com/x-api/introduction) official API.

We are keeping both options open in case Twikit stops working or Twitter's API becomes more restrictive. However, **Twikit has become the default way to run the project**.

If you use twitter's API, you can execute the project from the command line:

If you use Twikit, you can execute the project from the command line:

```bash
python3 twitter_search/run_twikit.py "location" "account_type" --skip_media "str"
```

```bash
python3 twitter_search "location" "account_type" --skip_media "str"
```

Where one will be looking for users in a certain location and from a certain account type. One can choose to skip the "media" type accounts if desired.

For example, if you want to get users from Kolkata in the "media" category with Twikit, you can use the following command:

```bash
python3 twitter_search/run_twikit "kolkata" "media"
```

The project will search Twitter based on the specified query and location, collecting user data and saving it in the raw data directory.

#### Generate .csv files for a particular city

If you want, it is also possible to generate csv files for a particular location. The command to generate Kolkata's .csv file for the 'Twikit' extracted users would be:

```bash
python3 twitter_search/generate_csv_files.py "kolkata" "True"
```

If you want to generate the .csv file for the 'Twitter API' extracted users, you can use the following command:

```bash
python3 twitter_search/generate_csv_files.py "kolkata" "False"
```

#### Concatenate all .csv files

There is another command we can use to concatenate all of the .csv files in the `cleaned_data` directory into a single file. This command is:

```bash
python3 twitter_search/concat_csv_files.py
```

Note that this command will also look for the .csvs generated by Twikit.

#### For a given location, get all the account types at once

If you want to get all the account types for a given location, you can use the following command:

```bash
python3 twitter_search/run_twikit.py "kolkata" "all" --skip_media "False"
```

Which will output all the account types, including media, for the given location (in this case, Kolkata). This will also generate the corresponding csv file for "Kolkata".


### For a given location, get all the account types at once, but skip _media_ account type.

During the project, we found that the _media_ account type outputs the most number of accounts, but it also captures a lot of irrelevant users. Therefore, to save resources and Twitter API calls, we added a skip_media parameter.

If you want to get all the account types for a given location, but skip the _media_ account type, you can use the following command:

```bash
python3 twitter_search/run_twikit.py "kolkata" "all" --skip_media "True"
```

Which will output all the account types for the given location (in this case, Kolkata), but skip the _media_ account type. This will also generate the corresponding csv file for "Kolkata".


## For all locations, get all the account types at once (not advisable since it will take a lot of time)

If you want to get all the account types for all locations, you can use the following command:

```bash
python3 twitter_search/run_twikit.py "all" "all"
```
Note that you can also use the `--skip_media` parameter for this command.

```bash
python3 twitter_search "all" "all" --skip_media "True"
```

## Get more users by using list expansion (WIP: create Twikit methods)

```bash
python3 twitter_search/list_expansion.py "bangalore" "researcher"
```

This will analyze the filtered users from bangalore that belong to the 'researcher' query (you must have already run the default script for the location and account type). It will then get all the lists that the users are a part of, filter the lists based on keywords, and then get all the users from the fitered lists.

TODO: The script will then filter the users based on location and content relevance.

Additionally, you can use the "all" parameter for the location to get all the account types for all locations. For example:

```bash
python3 twitter_search/list_expansion.py "all" "all"
```

You can also get all the account types for a given location by using the "all" parameter for the account type. For example:

```bash
python3 twitter_search/list_expansion.py "bangalore" "all"
```

## Run exploratory user and location analysis

The following command runs the exploratory analysis for the users and locations, which includes total number of users per location, total number of users per account type, etc.

```bash
python3 twitter_search/run_analysis.py
```

This step is crucial, in particular the location matching analysis, to determine which users reside in the location of interest.

## Graph / Networks Creation

As of April 2025, we are interested in representing the network of users in each particular city to identify the most 'central' users. We use the `analysis_outputs/location_matches.csv` file to get users whose location matches the desired city. Afterwards, we run the following processes:

### Network JSON Creation
This process creates a JSON with the followers / retweets per user in the desired location. Note that there will be several ways to get the users data:

- `"file"`: Reads the `location_matches.csv` file and gets the user_ids from the users who belong to such location.
- `"twikit"`: Searches for tweets mentioning the location, gets the users and only keeps the ones who are actually in the desired city (work in progress).
- `"x"`: Same approach as the twikit approach, but using the official X API (not implemented yet).

To get the network data for Kolkata users without waiting for 15 mins for the extraction to start, we run the script below. This process takes a long time since we need to wait 15 minutes, per user, so the rate limits are refreshed. Note that if a user is already present in the output JSON file, they will be skipped.

```bash
python3 twitter_search/run_network.py "kolkata" "twikit" "No"
```

The output data will be stored in the `data/networks/kolkata/kolkata.json`.

### Follower / Retweet Edges JSONs Creation
The second step uses the `data/networks/kolkata/kolkata.json` file to generate follower or retweeter based edges (the output will be either `data/networks/kolkata/follower_interactions.json` or `data/networks/kolkata/retweet_interactions.json`).

To create the follower-based edges for Kolkata, we do the following:

```bash
python3 twitter_search/run_edges.py "kolkata" "follower"
```
For the retweet-based edges, we run:

```bash
python3 twitter_search/run_edges.py "kolkata" "retweet"
```

### Graph Creation with NetworkX

Finally, we create the actual graph, along with its centrality calculations, with the following script:

For the retweet based graph:

```bash
python3 twitter_search/network/twitter_graph.py --location "kolkata"
```

For the follower based graph:

```bash
python3 twitter_search/network/twitter_graph.py --location "kolkata" --network-type "follower"
```

Note that two images will be created under the `data/networks/kolkata/visualizations` directory.


## Make Commands and addtional information

Note that, to add code to the repository, you will need to install pre-commit. This will ensure that the code is formatted correctly and that the tests pass before you commit. To install pre-commit, run the following command:

```bash
pip install pre-commit
```

You can then use the Makefile to format the code:

```bash
make lint
```

If you want to concatenate and run the exploratory analysis across the entire dataset

```bash
make concatenate_and_analyze
```

## Constants

Several constants are used in the code.

- Queries and account types: The available types of accounts, along with the queries used to search for the twitter users, are in the `config_utils/queries.py` file.

- Locations of interest: The locations of interest are in the `config_utils/cities.py` file. The `run.py` script iterates over these locations to search for users when the `all` argument is passed for the location.

- Access: The access keys are in the `config_utils/config.py` file. This tokens are used to access the Twitter API; you need to ask for this file from the authors.

- Other constants: Other constants are in the `config_utils/constants.py` file. These constants include paths, the number of users to fetch, capitals, the zero-shot threshold score and others.

- Constants for the `twitter_filtering code`: There are also some other constants in the `twitter_filtering/util/constants.py` file. These constants include paths, mostly.
